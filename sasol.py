# -*- coding: utf-8 -*-
"""sasol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_XiokgiHjVCMUryD5_JDiijtsHYsUC5j
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

test = pd.read_csv('/content/drive/MyDrive/Sasol/Test.csv')
test

train = pd.read_csv('/content/drive/MyDrive/Sasol/Train.csv')
train

defn = pd.read_csv('/content/drive/MyDrive/Sasol/VariableDescription.csv')
defn

x = train.drop('Target', axis=1)
x

y = train['Target']
y

x['region'].unique()

test['region'].unique()

x['tenure'].unique()

test['tenure'].unique()

x['top_pack'].unique()

test['top_pack'].unique()

# Correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(train.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

x.info()

y

#Encoding dummy variables
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# function to preprocess our data from train models
def preprocessing_data(x):

    # Convert the following numerical labels from interger to float
    float_array = x[ "regularity"].values.astype(float)

    # categorical features to be onverted to One Hot Encoding
    categ = ["region",
             "tenure",
             "mrg"
             ]

     # drop uniquid column
    x = x.drop(["ID"],axis=1)
    x = x.drop(["top_pack"],axis=1)
    # One Hot Encoding conversion
    x = pd.get_dummies(x, prefix_sep="_", columns=categ)

    # Label Encoder conversion
    #x["location_type"] = le.fit_transform(x["location_type"])
    #x["cellphone_access"] = le.fit_transform(x["cellphone_access"])
    #x["gender_of_respondent"] = le.fit_transform(x["gender_of_respondent"])

    x.fillna('0',inplace =True)

    scaler = MinMaxScaler(feature_range=(0, 1))
    x = scaler.fit_transform(x)

    return x

x = preprocessing_data(x)
x

x.info()





x

#Splitting the datset
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=0)

#import classifier algorithm here
from xgboost import XGBClassifier

# create models
xg_model = XGBClassifier()

#fitting the models
xg_model.fit(x_train,y_train)

y_pred = xg_model.predict(x_test)

# import evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score

# evaluate the model
xg_y_model = xg_model.predict(x_test)

# Get error rate
print("Error rate of XGB classifier: ", 1 - accuracy_score(y_test, xg_y_model))

100*accuracy_score(y_test, xg_y_model)

!pip install --upgrade scikit-learn

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={"size": 16})

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')

plt.show()

processed_test = preprocessing_data(test)

processed_test.shape

x.shape

# Get the predicted result for the test Data
test['Target'] = xg_model.predict(processed_test)

# Create submission DataFrame
submission = pd.DataFrame({"uniqueid": test["ID"], "Target": test['Target'] })

#show the five sample
submission.sample(5)

# Create submission csv file csv file
submission.to_csv('first_submission.csv', index = False)
files.download('first_submission.csv')

# Assuming you have imported the necessary modules
import pandas as pd
from google.colab import files  # Add this line

# Create submission DataFrame
submission = pd.DataFrame({"uniqueid": test["ID"], "Target": test['Target'] })

# Save to CSV file
submission.to_csv('first_submission.csv', index=False)

# Download the CSV file
files.download('first_submission.csv')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Assuming you have your features (X) and target variable (y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Create a Random Forest classifier with class_weight parameter
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)

# Fit the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))